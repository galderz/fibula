# fibula

`fibula-benchmarks` module enables end-user experience akin to JMH,
whereby users expect a single `benchmarks.jar` to execute.
It achieves that by depending solely on the `fibula-bootstrap` module,
which is the module where benchmark coordination begins,
and then making the module an `uber-jar`.
This effectively transforms the `fibula-bootstrap` module,
and all of its dependencies,
into a `benchmarks.jar` uber-jar.

Run individual benchmarks in JVM mode:
```shell script
make run BENCHMARK=JMHSample_01
```
Run individual benchmarks in Native mode:
```shell script
make run-native BENCHMARK=JMHSample_01
```
Running a benchmark with Linux perf stat profiler:
```shell script
make run-native BENCHMARK=JMHSample_01 PROFILER=perf
```
Decompile generated bytecode:
```shell script
make test DECOMPILE=true
```
Debugging benchmark generation for one of the tests:
```shell script
make test MAVEN_DEBUG=process GEN=BenchmarkStateOrderTest
```
Debugging benchmark test execution for one of the tests:
```shell script
make test MAVEN_DEBUG=test TEST=FailureModesTest
```
Running a benchmark sample in Native mode with perf stat profiler:
```shell script
make run-native BENCHMARK=JMHSample_01 PROF=perf
```
Running a benchmark sample in Native mode with perf stat or perf norm profiler with only branches and instructions
(needs https://github.com/galderz/jmh/commit/4000d778664e5a138fef8b8b79d3a823fa843527 to avoid multiplexing):
```shell script
make run-native BENCHMARK=JMHSample_01 PROF=perf:events=branches,instructions
make run-native BENCHMARK=JMHSample_01 PROF=perfnorm:events=branches,instructions
```
Running a benchmark sample in JVM mode with perf stat profiler:
```shell script
make run BENCHMARK=JMHSample_01 PROF=perf
```
Running a benchmark sample in Native mode with perfasm and saving the perf bin data:
```shell script
make run-native BENCHMARK=JMHSample_01 PROF=perfasm:skipAsm=true\\\;savePerfBin=true
```
Running a benchmark sample in Native mode with perfasm that uses DWARF call graph and saving the perf bin data
(needs https://github.com/galderz/jmh/commit/e623b6a93263fa24d5ec11076b59b3ea3b8dd0fc to enable modifiers):
```shell script
make run-native BENCHMARK=JMHSample_01 PROF=org.mendrugo.fibula.bootstrap.DwarfPerfAsmProfiler:events=cycles:P\\\;skipAsm=true\\\;savePerfBin=true DEBUG_INFO=true MEASURE_TIME=1 WARMUP_TIME=1
```

## TODO

- [ ] Print executing/results as TRACE and make results/params presented not in base64 format
- [ ] Separate building bootstrap+runner from running it (to avoid repeat build if source not changed)
- [ ] Avoid the BenchmarkList file and instead emit a build item and use the recorder
- [ ] Support running jvm mode with native image agent
      Doing this will require fixing the forking to avoid errors related to writing to same native image config file
      Once that is fixed, maybe default to running with only 1 fork and 1 warmup and 1 measurement fork, because that should be enough
- [ ] Add a record equals/hashCode benchmark
      Some potential examples:
      https://github.com/openjdk/jdk/blob/master/test/micro/org/openjdk/bench/java/io/RecordDeserialization.java
- [ ] See how record equals/hashCode benchmark behaves with different GraalVM versions
- [ ] Add profiler integration
      `perf annotate` is one option
      , another is to use `perf stat` along with `perf norm`
- [ ] Narrow down native issue with commons match dependency